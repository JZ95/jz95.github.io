---
title: Gentle Introduction to Variational Auto-Encoder 
date: 2022-05-06 20:08:23
---

## Intro

A Variational Auto-Encoder(VAE) is a widely used approach in unsupervised learning for complicated distributions. Though often associated with [auto-encoders](https://en.wikipedia.org/wiki/Autoencoder) in terms of the similarity in network architecture, a VAE's mathematical formulation is quite different from a auto-encoder. Therefore we are gonna talk about how VAE bridges the "variational" method and the "auto-encoder" theory as shown in its name. 

This blog is divided into two parts. The `PART I` will introduce you the problem scenario in which vae is designed for, 


compares 

In `PART II`, we will implement our own VAE model with pytorch and run the experiment on MNIST dataset.

You are always encouraged to check the original paper, [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114), for further detailed information. A google colab [notebook](https://colab.research.google.com/drive/1_fvinFbA70QDnI3idDjiVn-DlOGxs_Bx?usp=sharing) developed for `PART II` is open for readers to explore.



## PART I: Towards Variational AutoEncoder

### The Problem Scenario
We will start this blog by firstly explaining the connection VAEs have with the term -- "variational". Before that, however, we have to introduce the background of the story.

<!-- In [Bayesian Statistics](https://en.wikipedia.org/wiki/Bayesian_statistics), probability expresses a degree of belief in an event. The degree of belief may be based on prior knowledge, and will be updated after observing new data. In this section, we would explain the process about how "the degree of belief" being updated with a more concrete scenario. -->

Let's consider some dataset $\mathbf{X} = \\{ \mathbf{x}^{(i)} \\}_{i=1}^N$ consisting of $N$ i.i.d. samples of a random variable $\mathbf{x}$ (either scalar or vector). The data are assumed to be generated by some random process, involving an unobserved random variable $\mathbf{z}$ (i.e. the latent variable). 

The generative process has two steps: 
1. a value $\mathbf{z}^{(i)}$ is generated from some prior distribution $p_{\theta}(\mathbf{z})$; 
2. a value $\mathbf{x}^{(i)}$ is generated from some conditional distribution $p_{\theta}(\mathbf{x}|\mathbf{z}=\mathbf{z}^{(i)})$ dependent on $\mathbf{z}^{(i)}$;

where the prior $p_{\theta}(\mathbf{z})$ and likelihood $p_{\theta}(\mathbf{x}|\mathbf{z})$ are both parametric distributions of a unknown parameter set $\theta$.

We are interested in solving the following problems related to the given scenario:
1. the posterior inference of the latent variable $\mathbf{z}$ given an observed value $\mathbf{x}$ for a choice of parameters $\theta$, i.e. $p_\theta(\mathbf{z}|\mathbf{x})$, which is useful for representation learning.
2. the marginal inference of the variable $\mathbf{x}$, i.e. $p(\mathbf{x})$, which is useful in the scenarios where a prior over $\mathbf{x}$ is required.
3. the estimation for the parameter set $\theta$, with which one can mimic the above-mentioned generative process and create artificial data.

### Variational Methods
In this section, we will introduce you variational method used for solving the three problems. Now let us begin with the posterior inference problem, i.e. calculating $p_\theta(\mathbf{z}|\mathbf{x}=\mathbf{x}^{(i)})$. We can write down the posterior probability by applying Bayes's Theorem and probability chain's rule:
$$
\begin{aligned}
p(\mathbf{z}|\mathbf{x}) 
    & = \frac{p(\mathbf{z},\mathbf{x})}{p(\mathbf{x})} \\\\
    & = \frac{p(\mathbf{x}=\mathbf{x}^{(i)}|\mathbf{z}=\mathbf{z}^{(i)})p(\mathbf{z}=\mathbf{z}^{(i)})}{\int_{\mathbf{z}^{(i)}} p(\mathbf{x}=\mathbf{x}^{(i)}|\mathbf{z}=\mathbf{z}^{(i)})p(\mathbf{z}=\mathbf{z}^{(i)}) d\mathbf{z}^{(i)}} \\\\
    & = \frac{p(\mathbf{x}|\mathbf{z}) p(\mathbf{z})}{\int_{\mathbf{z}} p(\mathbf{x}|\mathbf{z}) p(\mathbf{z}) d\mathbf{z}} & \mathrm{simplify\ the\ notation}
\end{aligned}
$$

Since we have known the specification of the prior distribution $p_\theta(\mathbf{z})$ and the likelihood $p_\theta(\mathbf{x}^{(i)}|\mathbf{z})$ defined by the generative process, theoretically, the posterior $p_\theta(\mathbf{z}|\mathbf{x}^{(i)})$ can be calculated just by doing the integral $\int_{\mathbf{z}} p_\theta(\mathbf{x}^{(i)}|\mathbf{z}) p(\mathbf{z}) d\mathbf{z}$, which involves enumerating all the possible values the unobservable variable $\mathbf{z}$ may have.

However without any simplifying assumptions on $p_\theta(\mathbf{z}|\mathbf{x}^{(i)})$ or $p_\theta(\mathbf{z})$, the integral is intractable, which means the computation complexity of any approach for evaluating the integral including the enumeration operation is exponential. 

Variational methods are designed for such situations and allows us to avoid the intractable integral by transforming the inference problem to a optimization problem. In variational methods, a recognition model $q_\phi(\mathbf{z}|\mathbf{x}^{(i)})$ is proposed as an approximation to the true posterior $p_\theta(\mathbf{z}|\mathbf{x}^{(i)})$. By minimizing the [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between $q_\phi(\mathbf{z}|\mathbf{x}^{(i)})$ and $p_\theta(\mathbf{z}|\mathbf{x}^{(i)})$, we can solve the posterior inference problem. To simplify computation, both the parameters $\phi$ and $\theta$ of the recognition model and the generative model will be jointly optimized here.

$$\phi^*, \theta^* = \mathrm{argmax_{\phi, \theta}} \ KL\big(q_{\phi}(\mathbf{z}) ||p_\theta(\mathbf{z}|\mathbf{x}^{(i)})\big)$$

<!-- What a Bayesian statistician concerns about is how our belief about the set of unobserved variables $\mathbf{Z}=\\{\mathbf{z}^{(i)}\\}_{i=1}^N$ changed after seeing each value $\mathbf{x}^{(i)}$ within the dataset $\mathbf{X}$, i.e. the posterior probability $p(\mathbf{Z}|\mathbf{X})$. -->

<!-- We can write down the posterior probability by applying Bayes's Theorem and probability chain's rule:
$$
\begin{aligned}
p(\mathbf{Z}|\mathbf{X}) 
    & = \frac{p(\mathbf{Z},\mathbf{X})}{p(\mathbf{X})} = \frac{p(\mathbf{X}|\mathbf{Z}) p(\mathbf{Z})}{\prod_{i=1}^{N} p(\mathbf{x}=\mathbf{x}^{(i)})} \\\\
    & = \frac{\prod_{i=1}^{N} p(\mathbf{x}=\mathbf{x}^{(i)}|\mathbf{z}=\mathbf{z}^{(i)})p(\mathbf{z}=\mathbf{z}^{(i)})}{\prod_{i=1}^{N} \int_{\mathbf{z}^{(i)}} p(\mathbf{x}=\mathbf{x}^{(i)}|\mathbf{z}=\mathbf{z}^{(i)})p(\mathbf{z}=\mathbf{z}^{(i)}) d\mathbf{z}^{(i)}} \\\\
    & = \prod_{i=1}^{N} \frac{p(\mathbf{x}|\mathbf{z}) p(\mathbf{z})}{\int_{\mathbf{z}} p(\mathbf{x}|\mathbf{z}) p(\mathbf{z}) d\mathbf{z}} & \mathrm{simplify\ the\ notation}
\end{aligned}
$$ -->

<!-- The core problem by now is to solve the posterior distribution $p(\mathbf{z}|\mathbf{x}) = \frac{p(\mathbf{x}|\mathbf{z}) p(\mathbf{z})}{\int_{\mathbf{z}} p(\mathbf{x}|\mathbf{z}) p(\mathbf{z}) d\mathbf{z}}$, which is called ["inference"](https://en.wikipedia.org/wiki/Bayesian_inference) in Bayesian statistics.

 -->

Parameters ${\phi}$ and $\theta$ would be omitted for simplicity in the following deduction.

$$
\begin{aligned}
KL\big( q(\mathbf{z}|\mathbf{x}^{(i)})|| p(\mathbf{z}|\mathbf{x}^{(i)}) \big) & = \int_\mathbf{z} q(\mathbf{z}|\mathbf{x}) \mathrm{log}\frac{q(\mathbf{z}|\mathbf{x})}{p(\mathbf{z}|\mathbf{x})} d\mathbf{z} & \\\\
& = \mathbb{E_q}[\mathrm{log} q(\mathbf{z}|\mathbf{x})] - \mathbb{E_q}[\mathrm{log} p(\mathbf{z}|\mathbf{x})] & \mathrm{rewrite\ as\ the\ form\ of\ expectation} \\\\
& = \mathbb{E_q}[\mathrm{log} q(\mathbf{z}|\mathbf{x})] - \mathbb{E_q}[\mathrm{log} p(\mathbf{x}, \mathbf{z})] + {E_q}[\mathrm{log} p(\mathbf{x})] & p(\mathbf{z}|\mathbf{x}) = \frac{p(\mathbf{x}, \mathbf{z})}{p(\mathbf{x})} \\\\
& = \mathbb{E_q}[\mathrm{log} q(\mathbf{z}|\mathbf{x})] - \mathbb{E_q}[\mathrm{log} p(\mathbf{x}, \mathbf{z})] + \mathrm{log} p (\mathbf{x}) & p(\mathbf{x}) \mathrm{\ is\ irrelevant\ of\ } q(\mathbf{z}) \\\\
& = -\mathrm{ELBO}(q) + \mathrm{log} p (\mathbf{x})
\end{aligned}
$$


The term $\mathrm{log} p (\mathbf{x})$ is a constant, thus can be ignored during the optimization process. Furthermore, we rewrite the [evidence lower bound](https://en.wikipedia.org/wiki/Evidence_lower_bound), ELBO:
$$
\begin{aligned}
\mathrm{ELBO}(q)
& = - \mathbb{E_q}[\mathrm{log} q(\mathbf{z}|\mathbf{x})] + \mathbb{E_q}[\mathrm{log} p(\mathbf{x}, \mathbf{z})] \\\\
& = - \mathbb{E_q}[\mathrm{log} q(\mathbf{z}|\mathbf{x})] - \mathbb{E_q}[\mathrm{log} p(\mathbf{z})] + \mathbb{E_q}[\mathrm{log} p(\mathbf{x}| \mathbf{z})] \\\\
& = -KL\big(q(\mathbf{z}|\mathbf{x}) ||p(\mathbf{z})\big) + \mathbb{E_q}[\mathrm{log} p(\mathbf{x}| \mathbf{z})]
\end{aligned}
$$

The original optimization problem is by now equivalent to:

$$
\phi^*,\theta^* = \mathrm{argmax_{\phi, \theta}}\ -KL\big(q_{\phi}(\mathbf{z}|\mathbf{x}) ||p_\theta(\mathbf{z})\big) + \mathbb{E_{q_{\phi}(\mathbf{z}|\mathbf{x})}}[\mathrm{log} p_\theta(\mathbf{x}| \mathbf{z})]
$$

### The Learning Algorithm
In the previous section, with the help of variational method we get rid of the intractable integral in the posterior inference problem. By now the next challenge waiting for us is to decide what algorithm is used for solving the optimization problem. All three problems mentioned in the `The Problem Scenario` section will be solved if this challenge can be properly handled.

Just like other deep learning models, we sort to the stochastic gradient descent for solving the optimization problem. The objective function is
$$
\mathcal{L}(\phi,\theta, \mathbf{x^{(i)}}) = -KL\big(q_{\phi}(\mathbf{z}|\mathbf{x}^{(i)}) ||p_\theta(\mathbf{z})\big) + \frac{1}{L}\sum_{l=1}^L [\mathrm{log} p_\theta(\mathbf{x}^{(i)}| \mathbf{z}^{(i, l)})],
$$

where the expectation term is approximated by using Monte Carlo method, i.e. averaging the samples $\mathbf{z}^{(i, l)})$ drawn from $q_{\phi}(\mathbf{z}|\mathbf{x}^{(i)}$. With a differentiable objective function, the full stochastic gradient descent algorithm for VAE is as follows:
1. get the minibatch consisting of $M$ datapoints;
2. compute minibatch loss $\frac{1}{M}\sum_{i=1}^M\mathcal{L}(\phi,\theta, \mathbf{x^{(i)}})$;
3. compute gradients $\frac{1}{M}\sum_{i=1}^M \nabla_{\phi, \theta} \mathcal{L}(\phi,\theta, \mathbf{x^{(i)}})$;
4. update parameters $\phi, \theta$;
5. repeat the first 4 steps util convergence.


In practical, the samples $\mathbf{z}^{(i, l)})$ are not drawn directly from $q_{\phi}(\mathbf{z}|\mathbf{x}^{(i)}$, because $q$ can be arbitrary complicated distribution and difficult to sample. Therefore we sort to reparameterization trick by setting $\mathbf{z}^{(i, l)} = g_{\phi}(\epsilon^{(i;l)}; \mathbf{x}^{(i)})$, where $g_{\phi}$ can be any neural network which takes $\epsilon^{(i;l)}$ and $\mathbf{x}^{(i)}$ as input and the noise $\epsilon^{(i;l)}$ is sampled from some simple distribution $p(\epsilon)$ (e.g. gaussian).

### One More Step: Variational AutoEncoder
Finally, we make it to the "autoencoder" part in the journey of explaining the name of VAE. 

![arch](/images/vae/ae.png "auto-encoder")
In auto-encoders' world, a datapoint $\mathbf{x}^{(i)}$ is processed by the encoder $f(\mathbf{x})$ and then a code $\mathbf{z}^{(i)}$ is produced. The decoder $g(\mathbf{z})$ takes the code $\mathbf{z}^{(i)}$ as input and produces the reconstruction $\hat{\mathbf{x}}^{(i)}$. The reconstruction loss $\mathcal{L}(\mathbf{x}^{(i)})$, is often the squared error, $||\hat{\mathbf{x}}^{(i)} - \mathbf{x}^{(i)}||^2$.


![arch](/images/vae/vae.png "vae")
When it comes to VAEs, the unobserved variables $\mathbf{z}$ can be interpreted as the code. In addition, the recognition model $q_{\phi}(\mathbf{z}|\mathbf{x})$ can be treated as a probabilistic encoder, since given a datapoint $\mathbf{x}$ it produces a distribution over the possible values of $\mathbf{z}$, while $p_\theta(\mathbf{x}| \mathbf{z})$ can be seen as a probabilistic decoder, since given a code $\mathbf{z}$, it produces a distribution over the possible corresponding values of $\mathbf{x}$.

The log likelihood $\mathbb{E_{q_{\phi}(\mathbf{z}|\mathbf{x})}}[\mathrm{log} p_\theta(\mathbf{x}| \mathbf{z})]$ is used as the reconstruction loss. Besides, there is a KL divergence term in the objective function which acts as a regularizer and enforces the $q_{\phi}(\mathbf{z}|\mathbf{x}^{(i)}$ to stay close to the prior $p_{\theta}(\mathbf{z})$.

## PART II: VAE Practical
In this part, we will implement a VAE using Pytorch on our own according to the mathematics in the `PART I`, and run experiments on the MNIST dataset.

### a VAE for MNIST
For the MNIST task, the VAE model architecture is constructed as follows:
- gaussian encoder
  Due to its stable statical property and simplicity in sampling, we choose multi-variate gaussian as the encoder output distribution, where the mean and variance values are modelled by a feedforward network. 

    $$
    \begin{align}
    \mathbf{h} & = tanh(\mathbf{W_h}\mathbf{x} + \mathbf{b_h}) \\\\ 
    \mathbf{\mu} & = \mathbf{W_\mu}\mathbf{h} +  \mathbf{b_\mu}   \\\\
    \mathbf{\sigma}^2 &= exp(\mathbf{W_{\sigma^2}}\mathbf{h} +  \mathbf{b_{\sigma^2}}) \\\\
    q(\mathbf{z}|\mathbf{x}) & = \mathcal{N}(\mathbf{z}; \mathbf{\mu}, \mathbf{\sigma}^2 \mathbf{I})
    \end{align}
    $$

- bernoulli decoder
  The MNIST data are gray-scale images, of which each pixel can be represented as a float number between 0 and 1, therefore Bernoulli distribution become our natural choice for the decoder:
  $$
  \begin{align}
      \mathbf{h'} & = tanh(\mathbf{W_h'}\mathbf{z} + \mathbf{b_h'}) \\\\
      p(\mathbf{x}|\mathbf{z}) & = f_{\sigma}(\mathbf{W}\mathbf{h'} + \mathbf{b}) \\\\
  \end{align}
  $$
  where $f_{\sigma}$ is the sigmoid activation function.

![mnist](/images/vae/mnist.png "mnist-vae")

### Implementation by Pytorch
Here goes the pytorch code for implementing a VAE for the MNIST task.

```python
class GaussianEncoder(torch.nn.Module):
    """
    modelling the prob q(z|x), where z, x are n-d, m-d vectors
    """
    def __init__(self, dim_z, dim_x, dim_hidden):
        super(GaussianEncoder, self).__init__()
        self.hidden_layer = torch.nn.Sequential(
                                torch.nn.Linear(dim_x, dim_hidden), 
                                torch.nn.Tanh()
                                )
        
        # transform hidden vector to gaussian mean
        self.mean_transform_layer = torch.nn.Linear(dim_hidden, dim_z)

        # transform hidden vector to gaussian variance
        self.var_transform_layer = torch.nn.Linear(dim_hidden, dim_z)

    def get_mean_and_var(self, x):
        """
        :param x: the condition part, [batch, m]
        :return: (mean, variance) [batch, n], [batch, n]
        """

        h = self.hidden_layer(x)  # [batch, h]
        return (self.mean_transform_layer(h), 
                    torch.exp(self.var_transform_layer(h)))

    def forward(self, z, x):
        """
        give the log prob of p(z|x)
        :param z: [batch, n]
        :param x: [batch, m]
        :return: [batch, ]
        """
        dim_z = z.shape[1]
        mean, var = self.get_mean_and_var(x)  # [batch, n], [batch, n]
        
        # inversed covariance mat, [b, n, n]
        inv_covar = torch.einsum('bi, ij -> bij', 
                                 1 / var, 
                                 torch.eye(dim_a))
        # gaussian pdf
        exponent = - 1 / 2 * torch.einsum('bi, bi -> b', 
                                          torch.einsum('bi, bij->bj', 
                                                        a - mean, 
                                                        inv_covar), 
                                          a - mean)  # [b,]

        return - dim_a / 2 * torch.log(torch.tensor(2 * torch.pi)) \
               - 1 / 2 * torch.sum(torch.log(var), dim=1) + exponent

    def generate(self, b):
        """
        :param b: [batch, dim_b]
        :return: [batch, dim_a]
        """
        with torch.no_grad():
            mean, var = self.get_mean_and_var(b)
            return mean + torch.sqrt(var) * torch.randn(var.shape)
```


```python
class BernoulliDecoder(torch.nn.Module):
    """
    The decoder modelling likelihood p(x|z),
    suitable for binary-valued data, or the real-value between 0 and 1
    described in the Appendix C
    """
    def __init__(self, dim_latent, dim_input, dim_hidden):
        super(BernoulliDecoder, self).__init__()
        self.layer = torch.nn.Sequential(
            torch.nn.Linear(dim_latent, dim_hidden),
            torch.nn.Tanh(),
            torch.nn.Linear(dim_hidden, dim_input),
            torch.nn.Sigmoid()
        )

    def forward(self, x, z):
        """
        evaluate the log - prob of p(x|z)
        :param x: [batch, n]
        :param z: the given latent variables, [b, m]
        :return: [batch, ]
        """
        y = self.layer(z)  # [b, n]
        return torch.sum(x * torch.log(y) + (1 - x) * torch.log(1 - y), dim=1)

    def generate(self, z):
        """
        generate data points given the latent variables, i.e. draw x ~ p(x|z)
        :param z: the given latent variables, [batch, m]
        :return: generated data points, [batch, n]
        """
        with torch.no_grad():
            # [batch, n]
            y = self.layer(z)
            return torch.where(torch.rand(y.shape) > y, 0., 1.)

    def prob(self, z):
        """
        evaluate the conditional probability
        :param z: the given latent variables, [batch, m]
        :return: [batch, n], 0 <= elem <= 1
        """
        with torch.no_grad():
            return self.layer(z)
```

Below is the full code for the VAE model that consists of a gaussian encoder and a bernoulli decoder.

```python
class VAEModel(torch.nn.Module):
    """
    the variational auto-encoder for MNIST data, see the original paper for reference:
    https://arxiv.org/abs/1312.6114
    """
    def __init__(self, dim_latent, dim_input, dim_hidden):
        super(VAEModel, self).__init__()
        self.encoder = GaussianMLP(dim_latent, dim_input, dim_hidden)
        self.decoder = BernoulliDecoder(dim_latent, dim_input, dim_hidden)

    def compute_loss(self, data, reduction='mean'):
        if reduction == 'mean':
            return - torch.mean(self.forward(data))
        elif reduction == 'sum':
            return - torch.sum(self.forward(data))


    def forward(self, x) -> torch.Tensor:
        """
        corresponds to equation (10) in the original paper
        :return: the estimated ELBO value, i.e. the objective function
        """
        mean, var = self.encoder.get_mean_and_var(x)  # [b, n], [b, n]
        z = mean + torch.sqrt(var) * torch.randn(var.shape).to(x.device)
        # the KL divergence term plus the MC estimate of decoder
        return 1 / 2 * torch.sum(1 + torch.log(var) - mean ** 2 - var, dim=1) + self.decoder(x, z)
```

### Experiment on MNIST dataset
We tested the convergence of our VAE on MNIST dataset.

Also, we analyzed the 2D manifold learned from the given data.


google colab [notebook](https://colab.research.google.com/drive/1_fvinFbA70QDnI3idDjiVn-DlOGxs_Bx?usp=sharing)